{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "credito = pd.read_csv('../data/Credit.csv')\n",
    "previsores = credito.iloc[:,0:20].valuesFundamentos de Data Science, Data Mining e Análise\n",
    "Preditiva\n",
    "Especialização em Ciênciade Dados com Big Data, BI e Data\n",
    "Analytics\n",
    "Prof. Dr. Carlos Barros\n",
    "Princípios data mining\n",
    "● Introdução a Data Mining e Ciência dos Dados.\n",
    "Obtendo informações a partir dos dados. Principais\n",
    "Paradigmas e Modelos para mineração de dados.\n",
    "Dados incertos, com ruídos/outliers e confiança\n",
    "nos dados. Análise de dados exploratória.\n",
    "Introdução ao uso de modelos de predição.\n",
    "Escolha de modelos para mineração de dados.\n",
    "Redução de dimensionalidade e engenharia de\n",
    "dados. Minerando dados complexos. Trabalhando\n",
    "e limpando os Dados.\n",
    "Redução de dimensionalidade\n",
    "e engenharia de dados.\n",
    "Minerando dados complexos.\n",
    "Trabalhando e limpando os\n",
    "Dados\n",
    "Engenheiro de Dados\n",
    "O Engenheiro de Dados é o responsável pela criação do pipeline que transforma os dados\n",
    "brutos que estão nos mais variados formatos, desde bancos de dados transacionais até\n",
    "arquivos de texto, em um formato que permita ao Cientista de Dados começar seu\n",
    "trabalho. Cabe também ao Engenheiro de Dados manter este pipeline em execução para\n",
    "que os dados possam ser coletados no momento certo, com o nível se segurança exigido\n",
    "pela empresa. O trabalho do Engenheiro de dados é tão importante quanto o trabalho do\n",
    "Cientista de Dados, mas eles costumam ter menos visibilidade, uma vez que estão mais\n",
    "distantes do produto final resultado do processo de análise, o que é produzido pelo\n",
    "Cientista de Dados.\n",
    "Imagine uma aplicação e sua arquitetura de dados\n",
    "• Entrada – isso envolve a coleta de dados necessários.\n",
    "• Processamento – isso envolve o processamento dos dados para\n",
    "obter os resultados finais desejados.\n",
    "• Armazenamento – isso envolve armazenar os resultados finais\n",
    "para recuperação rápida.\n",
    "• Acesso – você precisará habilitar uma ferramenta ou usuário\n",
    "para acessar os resultados finais do pipeline.\n",
    "Engenheiro de Dados\n",
    "Habilidades do Engenheiro de Dados\n",
    "Um Engenheiro de Dados precisa ser bom em:\n",
    "• Arquitetar sistemas distribuídos\n",
    "• Criar pipelines confiáveis\n",
    "• Combinar fontes de dados\n",
    "• Criar a arquitetura de soluções\n",
    "• Colaborar com a equipe de Data Science e construir as soluções certas para essas\n",
    "equipes\n",
    "Redução da dimensionalidade\n",
    "Termo dimensionalidade é atribuído ao número de\n",
    "características de uma representação de padrões, ou seja, a\n",
    "dimensão do espaço de características. As duas principais razões\n",
    "para que a dimensionalidade seja a menor possível são: custo\n",
    "de medição e precisão do classificador. Quando o espaço de\n",
    "características contém somente as características mais salientes,\n",
    "o classificador será mais rápido e ocupará menos memória.\n",
    "Com o aumento horizontal das bases de dados (dimensões / atributos) um problema\n",
    "grave é o aumento da dimensionalidade (Course of Dimensionality) em que temos não\n",
    "somente multicolinearidade, heteroscedasticidade e autocorreação para ficar em\n",
    "exemplos estatísticos simples. Em termos computacionais nem é preciso dizer que o\n",
    "aumento de atributos faz com que os algoritmos de Data Mining ou Inteligência\n",
    "Computacional tenham que processar um volume de dados muito maior (aumento da\n",
    "complexidade do processamento = maior custo temporal).\n",
    "Redução da dimensionalidade\n",
    "Redução da dimensionalidade – Principais técnicas\n",
    "Missing Values Ratio.\n",
    "Low Variance Filter\n",
    "High Correlation Filter\n",
    "Random Forests / Ensemble Trees\n",
    "Principal Component Analysis (PCA)\n",
    "Backward Feature Elimination.\n",
    "Forward Feature Construction\n",
    "Redução da dimensionalidade\n",
    "knime_seventechniquesdatadimreduction\n",
    "Apesar da robustez matemática, o PCA\n",
    "apresenta um resultado não tão satisfatório\n",
    "em relação a métodos mais simples de seleção\n",
    "de atributos. Isso pode indicar que esse\n",
    "método não lida tão bem com bases de dados\n",
    "com inconsistências. Filtro de baixa variância e\n",
    "de valores faltantes são técnicas\n",
    "absolutamente simples e tiveram o mesmo\n",
    "resultado de técnicas algoritmicamente mais\n",
    "complexas como Florestas Aleatórias.\n",
    "Construção de modelos com inclusão\n",
    "incremental de atributos e eliminação de\n",
    "atributos retroativa são métodos que\n",
    "apresentam uma menor performance e são\n",
    "proibitivos em termos de processamento.\n",
    "A estatística básica ainda é uma grande\n",
    "ferramenta para qualquer data miner, e não\n",
    "somente ajuda em termos de redução do custo\n",
    "temporal (processamento) quanto em custo\n",
    "espacial (custo de armazenamento).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Dados complexos – dados estr & não estruturados (80%)\n",
    "dados multimídia (imagens, video, audio) – netlfix, clinicas, tv, radios\n",
    "dados geográficos -\n",
    "dados temporais - funceme\n",
    "Dados do genoma -\n",
    "Textos\n",
    "O LHC (Large Hadron Collider) é um acelerador de partículas instalado próximo\n",
    "da fronteira entre Suíça e França. Ele contém quatro detetores de partículas que\n",
    "registram 40 milhões de eventos por segundo, registrados por 150 milhões de sensores.\n",
    "O volume de dados pré-processados é aproximadamente igual a 27 terabytes\n",
    "por dia.\n",
    "O Instituto Nacional de Pesquisas Espaciais tem uma base de dados de imagens de\n",
    "satélite com mais de 130 terabytes [29].\n",
    "O projeto Internet Archive3 mantém um arquivo de diversos tipos de mídia, contendo\n",
    "2 petabytes e crescendo cerca de 20 terabytes por mês, com aproximadamente\n",
    "130.000 vídeos, 330.000 arquivos de áudio, quase 500.000 documentos de texto e\n",
    "indexando 85 bilhões de páginas em várias versões.\n",
    "De acordo com algumas estimativas4, o site YouTube continha 45 terabytes de\n",
    "vídeos em 2006. O site Flickr tinha 2 bilhões de fotografias digitais5 em 2007 (e\n",
    "um teste rápido mostrou que já são ao menos 2.2 bilhões). Considerando que uma\n",
    "imagem, suas variantes criadas pelo site e outros dados como comentários ocupem\n",
    "um mínimo de 300 kilobytes, toda a coleção usa mais de 614 terabytes no total.\n",
    "O banco de dados GenBank contém coleções anotadas de sequências de nucleotídeos\n",
    "e proteínas de mais de 100.000 organismos, em um total de 360 gigabytes.\n",
    "O Large Synoptic Survey Telescope contém uma câmera digital de aproximadamente\n",
    "3.2 gigapixels e deve coletar 20 a 30 terabytes de imagens por noite.\n",
    "Um levantamento feito pelaWinter Corporation9 menciona algumas bases de dados\n",
    "de grande porte em uso (em 2005): Yahoo! (100 terabytes), AT&T (93 terabytes),\n",
    "Amazon (24 terabytes), Cingular (25 terabytes).\n",
    "Minenação de textos\n",
    "Análise de sentimentos\n",
    "Classificação de documentos\n",
    "detecção de fraudes\n",
    "Anúncios contextualizados\n",
    "Filtro de Spam\n",
    "\n",
    "classe = credito.iloc[:,20].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelencoder = LabelEncoder()\n",
    "previsores[:,0] = labelencoder.fit_transform(previsores[:,0])\n",
    "previsores[:,2] = labelencoder.fit_transform(previsores[:,2])\n",
    "previsores[:, 3] = labelencoder.fit_transform(previsores[:, 3])\n",
    "previsores[:, 5] = labelencoder.fit_transform(previsores[:, 5])\n",
    "previsores[:, 6] = labelencoder.fit_transform(previsores[:, 6])\n",
    "previsores[:, 8] = labelencoder.fit_transform(previsores[:, 8])\n",
    "previsores[:, 9] = labelencoder.fit_transform(previsores[:, 9])\n",
    "previsores[:, 11] = labelencoder.fit_transform(previsores[:, 11])\n",
    "previsores[:, 13] = labelencoder.fit_transform(previsores[:, 13])\n",
    "previsores[:, 14] = labelencoder.fit_transform(previsores[:, 14])\n",
    "previsores[:, 16] = labelencoder.fit_transform(previsores[:, 16])\n",
    "previsores[:, 18] = labelencoder.fit_transform(previsores[:, 18])\n",
    "previsores[:, 19] = labelencoder.fit_transform(previsores[:, 19])\n",
    "classe = labelencoder.fit_transform(classe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_treinamento, X_teste, y_treinamento, y_teste = train_test_split(previsores,\n",
    "                                                                  classe,\n",
    "                                                                  test_size = 0.3,\n",
    "                                                                  random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(gamma='auto')\n",
    "svm.fit(X_treinamento, y_treinamento)\n",
    "previsoes = svm.predict(X_teste)\n",
    "taxa_acerto = accuracy_score(y_teste, previsoes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = ExtraTreesClassifier(n_estimators=100)\n",
    "forest.fit(X_treinamento, y_treinamento)\n",
    "importancias = forest.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7733333333333333"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#floresta aleatoria\n",
    "forest2 = RandomForestClassifier(n_estimators=100 , criterion = 'entropy', random_state = 0)\n",
    "forest2.fit(X_treinamento, y_treinamento)\n",
    "previsoes_forest2 = forest2.predict(X_teste)\n",
    "taxa_acerto_forest = accuracy_score(y_teste, previsoes_forest2)\n",
    "importancias2 = forest2.feature_importances_\n",
    "taxa_acerto_forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0 1 5 7\n",
    "X_treinamento2 = X_treinamento[:,[0,4,12]]\n",
    "X_teste2 = X_teste[:,[0,4,12]]\n",
    "\n",
    "svm2 = SVC(gamma='auto')\n",
    "svm2.fit(X_treinamento2, y_treinamento)\n",
    "previsoes2 = svm2.predict(X_teste2)\n",
    "taxa_acerto = accuracy_score(y_teste, previsoes2)\n",
    "taxa_acerto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexssandroos/Documents/datascience/venv/lib/python3.7/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype object was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.66666667, 0.02941176, 0.25      , ..., 0.        , 1.        ,\n",
       "        1.        ],\n",
       "       [0.33333333, 0.64705882, 0.75      , ..., 0.        , 0.        ,\n",
       "        1.        ],\n",
       "       [0.        , 0.11764706, 0.25      , ..., 1.        , 0.        ,\n",
       "        1.        ],\n",
       "       ...,\n",
       "       [0.        , 0.11764706, 0.75      , ..., 0.        , 0.        ,\n",
       "        1.        ],\n",
       "       [0.66666667, 0.60294118, 0.75      , ..., 0.        , 1.        ,\n",
       "        1.        ],\n",
       "       [0.33333333, 0.60294118, 0.25      , ..., 0.        , 0.        ,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(previsores)\n",
    "scaler.transform(previsores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 1.\n 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0.\n 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 0. 1.\n 1. 0. 1. 0. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1.\n 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1.\n 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 1. 1.\n 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1.\n 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1.\n 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 0.\n 1. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1.\n 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0.\n 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0.\n 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0.\n 1. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1.\n 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0. 0.\n 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0.\n 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0.\n 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0.\n 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0.\n 1. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1.\n 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 0. 1. 0.\n 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0.\n 0. 1. 1. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1.\n 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0.\n 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1.\n 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0.\n 0. 1. 0. 1.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-bb458f4c92c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprevisores_treinamento_pca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_treinamento\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprevisores_teste_pca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_treinamento\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mcomponentes_pca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplained_variance_ratio_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/datascience/venv/lib/python3.7/site-packages/sklearn/decomposition/pca.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \"\"\"\n\u001b[0;32m--> 359\u001b[0;31m         \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m         \u001b[0mU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_components_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/datascience/venv/lib/python3.7/site-packages/sklearn/decomposition/pca.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         X = check_array(X, dtype=[np.float64, np.float32], ensure_2d=True,\n\u001b[0;32m--> 381\u001b[0;31m                         copy=self.copy)\n\u001b[0m\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;31m# Handle n_components==None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/datascience/venv/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    550\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m                     \u001b[0;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;31m# in the future np.flexible dtypes will be handled like object dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 1.\n 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0.\n 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 0. 1.\n 1. 0. 1. 0. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1.\n 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1.\n 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 1. 1.\n 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1.\n 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1.\n 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 0.\n 1. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1.\n 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0.\n 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0.\n 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0.\n 1. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1.\n 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0. 0.\n 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0.\n 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0.\n 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0.\n 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0.\n 1. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1.\n 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 0. 1. 0.\n 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0.\n 0. 1. 1. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1.\n 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0.\n 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1.\n 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0.\n 0. 1. 0. 1.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components = 6)\n",
    "previsores_treinamento_pca = pca.fit_transform(X_treinamento)\n",
    "previsores_teste_pca = pca.fit_transform(y_treinamento)\n",
    "componentes_pca = pca.explained_variance_ratio_\n",
    "\n",
    "# 0 1 5 7\n",
    "X_treinamento_pca= previsores_treinamento_pca\n",
    "X_teste_pca = previsores_teste_pca\n",
    "\n",
    "svm_pca = SVC(gamma='auto')\n",
    "svm_pca.fit(X_treinamento_pca, X_teste_pca)\n",
    "previsoes_pca = svm2.predict(X_teste_pca)\n",
    "taxa_acerto_pca = accuracy_score(y_teste, previsoes_pca)\n",
    "taxa_acerto_pca"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
